# to execute the workload on your local machine set endpoint qParam executeOnLocalMachine=Y	
# if your local machine is not powerful enough then utilize this Cloud resource
# 1- login/signup   	  		   https://auth.instill.tech/
# 2- create/use tokens 			   https://instill.tech/settings/api-tokens
# 3- update application.properties cloud.resource.token=<YOUR_TOKEN>
# 4- when invoking the endpoint ensure that the executeOnLocalMachine=   is blank
cloud.resource.token=<YOUR-TOKEN-HERE>

#provide the specific model name that you chose when executing the command   ollama run
llm.model.name=llava

llm.response.temperature=0
llm.server.port=11434
server.port=8888
llm.system.message=You are a helpful assistant.